SERVICE_NAME=STORAGE-SVC-WORK-ORDER

logger.scala.slick.session=DEBUG
logger.scala.slick=DEBUG

db.default.logSql=true

# These settings are read by slick.util.SlickConfig
# See: https://scala-slick.org/doc/3.3.2/config.html
slick {
  # Use ANSI color sequences in tree dumps (as used in log messages and
  # exceptions of type SlickTreeException).
  ansiDump = false

  # Use Unicode box characters in tree and table dumps (as used in log
  # messages). If this option is not enabled, ASCII approximations are used
  # instead.
  unicodeDump = false

  # Dump individual "Select" and "Ref" nodes instead of combining them into a
  # single "Path" element when creating a tree dump (as used in log messages
  # and exceptions of type SlickTreeException).
  dumpPaths = false

  # Use multi-line, indented formatting for SQL statements. If not enabled,
  # statements are generated without any linebreaks or indentation. This
  # option applies to all generated statements (but not to the Plain SQL API).
  sqlIndent = true

  # Verify types after each query compiler phase. This is useful for debugging
  # the query compiler but should generally not be enabled in production
  # environments because it makes query compilation considerably slower.
  verifyTypes = false

  # Detect unnecessary rebuilding of the AST after every query compiler phase.
  # This is useful for debugging the query compiler. Query compilation
  # performance is negatively affected by this if phase logging is enabled.
  detectRebuild = false
}

// Defaults
POSTGRES_SERVER = "localhost"
POSTGRES_DATABASE_NAME = "caeadom"
POSTGRES_PORT = "25432"
POSTGRES_USER = "caeadom"
POSTGRES_PASSWORD = ""

database = {
  profile = "slick.jdbc.PostgresProfile$"
  // Simple datasource with no connection pooling. The connection pool has
  // already been specified with HikariCP.
  dataSourceClass = "org.postgresql.ds.PGSimpleDataSource"
  db {
    connectionPool = "HikariCP" //use HikariCP for our connection pool or "disabled"
    poolName = WorkOrderStoragePool
    keepAliveConnection = true

    # set postgres URL. Use stringtype=unspecified to indicate strings should be
    # inferred. Remove problem with storing JSON string into JSONB column
    url = "jdbc:postgresql://"${?POSTGRES_SERVER}":"${?POSTGRES_PORT}"/"${?POSTGRES_DATABASE_NAME}"?stringtype=unspecified&currentSchema=datafabric_workorder"

    user = ${?POSTGRES_USER}
    password = ${?POSTGRES_PASSWORD}
    host = ${?POSTGRES_SERVER}
    port = ${?POSTGRES_PORT}
    databaseName = ${?POSTGRES_DATABASE_NAME}
  }
}

// Generic Kafka topic config

// Defaulting to the development instance using "dev-tools/docker-compose.yml"
KAFKA_BOOTSTRAP_SERVERS="localhost:29092"
KAFKA_CONSUMER_FILTER_STRING=Test

// Storage service kafka consumer/producer properties.
kafka.inbound.work.order.topic=default.datafabric.transform.work.order
kafka.outbound.work.order.topic=default.datafabric.storage.work.order
kafka.inbound.work.order.template.topic=default.datafabric.transform.work.order.template
kafka.outbound.work.order.template.topic=default.datafabric.storage.work.order.template
kafka.inbound.work.order.dependency.topic=default.datafabric.transform.work.order.dependency
kafka.outbound.work.order.dependency.topic=default.datafabric.storage.work.order.dependency
kafka.inbound.work.order.template.dependency.topic=default.datafabric.transform.work.order.template.dependency
kafka.outbound.work.order.template.dependency.topic=default.datafabric.storage.work.order.template.dependency

kafka.storage.concurrent.consumers=4

kafka.storage.consumer.workorder.groupId=work-order-storage
kafka.storage.consumer.workordertemplate.groupId=work-order-template-storage
kafka.storage.consumer.workorderdependency.groupId=work-order-dependency-storage
kafka.storage.consumer.workordertemplatedependency.groupId=work-order-template-dependency-storage
kafka.storage.consumer.max.partition.fetch.bytes=1000000
kafka.storage.consumer.fetch.max.bytes=1000000
kafka.storage.consumer.auto-offset-reset=earliest
kafka.storage.consumer.session.timeout=15000
kafka.storage.consumer.retry.interval=1000
kafka.storage.consumer.retry.max.interval=30000
kafka.storage.consumer.retry.multiplier=2
kafka.storage.consumer.retry.dlt.publish=true
kafka.storage.consumer.default.retry.limit=10
kafka.storage.consumer.retry.limit.map="{'org.springframework.transaction.CannotCreateTransactionException': '-1','org.springframework.dao.DataIntegrityViolationException': '0','com.abb.es.df.common.exception.MessageTransformationException': '0','com.abb.es.df.common.exception.UnrecoverableMessageStorageException': '0','org.springframework.dao.InvalidDataAccessApiUsageException': '0','org.springframework.messaging.MessageHandlingException':'0'}"
